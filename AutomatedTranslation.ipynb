{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PetyoKaratov/NLP-Task/blob/main/AutomatedTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTJS9RQp7mmj"
      },
      "source": [
        "# Automated Translation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOntjN1q7mmu"
      },
      "source": [
        "\n",
        "## Importing the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install trax\n",
        "! pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7DoODuN8MpP",
        "outputId": "489e0d93-9792-410b-f5f6-7d62a2b2be32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (1.2.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.14+cuda11.cudnn805)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.21.6)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.14)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->trax) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->trax) (3.8.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax->trax) (0.7.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax->trax) (5.9.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.5.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.64.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.17.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.10.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.56.4)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (14.0.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (2.9.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.47.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (21.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text->trax) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3BYOv3u7mmw",
        "outputId": "e5551926-ec17-4d7b-9214-529d4be222fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trax                          1.4.1\n"
          ]
        }
      ],
      "source": [
        "from termcolor import colored\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training\n",
        "import sentencepiece as spm\n",
        "!pip list | grep trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JIlR2M27mm0",
        "outputId": "6d915c6d-1281-408d-f4e1-f1dacf364165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:520: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        }
      ],
      "source": [
        "# Get generator function for the training set\n",
        "# This will download the train dataset if no data_dir is specified.\n",
        "train_stream_fn = trax.data.TFDS('para_crawl/enro',\n",
        "                                 data_dir='./data/',\n",
        "                                 keys=('en', 'ro'),\n",
        "                                 eval_holdout_size=0.01, # 1% for eval\n",
        "                                 train=True)\n",
        "\n",
        "# Get generator function for the eval set\n",
        "eval_stream_fn = trax.data.TFDS('para_crawl/enro',\n",
        "                                data_dir='./data/',\n",
        "                                keys=('en', 'ro'),\n",
        "                                eval_holdout_size=0.01, # 1% for eval\n",
        "                                train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln-5wJec7mm3",
        "outputId": "7f739d2c-370d-409d-dda8-bbe858700df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mtrain data (en, ro) tuple:\u001b[0m (b'Transfer Beaune Paris airport Charles de Gaulle, Orly, Beauvais, Le Bourget Taxi Beaune', b'Transfer Beaune Paris aeroportul Charles de Gaulle, Orly, Beauvais, Le Bourget Taxi Beaune la Paris - Cum se poate ajunge')\n",
            "\n",
            "\u001b[31meval data (en, ro) tuple:\u001b[0m (b'16 And your house shall be faithful, and your kingdom shall be before your face, for eternity, and your throne shall be secure continuously.\\xe2\\x80\\x99 \\xe2\\x80\\x9d', b'16 Casa ta va fi neclintit\\xc4\\x83, regatul t\\xc4\\x83u va r\\xc4\\x83m\\xc3\\xa2ne ve\\xc5\\x9fnic \\xc3\\xaenaintea ta \\xc5\\x9fi tronul t\\xc4\\x83u va sta \\xc3\\xaen veci\".')\n"
          ]
        }
      ],
      "source": [
        "train_stream = train_stream_fn()\n",
        "print(colored('train data (en, ro) tuple:', 'red'), next(train_stream))\n",
        "print()\n",
        "\n",
        "eval_stream = eval_stream_fn()\n",
        "print(colored('eval data (en, ro) tuple:', 'red'), next(eval_stream))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov4w18Rh3EQj",
        "outputId": "a4a13ef7-9dc0-4f08-8b98-fd4072ffbe41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = ['/content/drive/MyDrive/setimes_lexacctrain.en', '/content/drive/MyDrive/setimes_lexacctrain.ro']\n",
        "with open('/content/drive/MyDrive/setimes_lexacctrain.enro', 'w') as outfile:\n",
        "    for fname in filenames:\n",
        "        with open(fname) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)"
      ],
      "metadata": {
        "id": "aZB0LM8L6SZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python data/text_encoder_build_subword.py \\\n",
        "--corpus_filepattern=/content/drive/MyDrive/setimes_lexacctrain.enro --corpus_max_lines=32000 \\\n",
        "--output_filename=data/enro_32k.subword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HPV7Mqcft_k",
        "outputId": "b6c45e5b-af7f-41ea-84e3-2465742a7e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I0819 23:57:46.369189 139929510954880 text_encoder.py:796] Iteration 0\n",
            "I0819 23:57:47.219192 139929510954880 text_encoder.py:861] vocab_size = 39099\n",
            "I0819 23:57:47.219414 139929510954880 text_encoder.py:796] Iteration 1\n",
            "I0819 23:57:47.578960 139929510954880 text_encoder.py:861] vocab_size = 15291\n",
            "I0819 23:57:47.579158 139929510954880 text_encoder.py:796] Iteration 2\n",
            "I0819 23:57:47.933028 139929510954880 text_encoder.py:861] vocab_size = 15541\n",
            "I0819 23:57:47.933227 139929510954880 text_encoder.py:796] Iteration 3\n",
            "I0819 23:57:48.286776 139929510954880 text_encoder.py:861] vocab_size = 15494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIOAP_X7mm4"
      },
      "source": [
        "\n",
        "## Tokenization and Formatting\n",
        "\n",
        "Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBMpxP2r7mm6"
      },
      "outputs": [],
      "source": [
        "# global variables that state the filename and directory of the vocabulary file\n",
        "VOCAB_FILE = 'enro_32k.subword'\n",
        "VOCAB_DIR = 'data/'\n",
        "\n",
        "# Tokenize the dataset.\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQBSrFHO7mm7"
      },
      "source": [
        "**Append an end-of-sentence token to each sentence:** We will assign a token (i.e. in this case `1`) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAAF86t7mm7"
      },
      "outputs": [],
      "source": [
        "# Append EOS at the end of each sentence.\n",
        "\n",
        "# Integer assigned as end-of-sentence (EOS)\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "# append EOS to the train data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "\n",
        "# append EOS to the eval data\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3FwQZ3u7mm8"
      },
      "source": [
        "**Filter long sentences:** We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the `trax.data.FilterByLength()` method and you can see its syntax below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2hZlXVI7mm8",
        "outputId": "d4212894-a831-4257-a2e9-796884778211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mSingle tokenized example input:\u001b[0m [7460 2554    2 7097 6369 5617 3430   14  120  742    4    1]\n",
            "\u001b[31mSingle tokenized example target:\u001b[0m [ 7025  7245  9041 15481  5751  7030 12618    23  3398  1084  3634  5432\n",
            "  6777  6534  9041 15481  5751  7030 12618    23  7290  9016 15481  5750\n",
            "  4665 12618  2793  6012 15481  5750  4665 12618  3869     4     1]\n"
          ]
        }
      ],
      "source": [
        "# Filter too long sentences to not run out of memory.\n",
        "# length_keys=[0, 1] means we filter both English and German sentences, so\n",
        "# both much be not longer that 256 tokens for training / 512 for eval.\n",
        "filtered_train_stream = trax.data.FilterByLength(\n",
        "    max_length=256, length_keys=[0, 1])(tokenized_train_stream)\n",
        "filtered_eval_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
        "\n",
        "# print a sample input-target pair of tokenized sentences\n",
        "train_input, train_target = next(filtered_train_stream)\n",
        "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
        "print(colored(f'Single tokenized example target:', 'red'), train_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG6b6w9f7mm9"
      },
      "source": [
        "<a name=\"1.3\"></a>\n",
        "## 1.3  tokenize & detokenize helper functions\n",
        "\n",
        "Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
        "\n",
        "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
        "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
        "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
        "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
        "\n",
        "\n",
        "- <span style='color:blue'> tokenize(): </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords (parts of words).\n",
        "- <span style='color:blue'> detokenize(): </span> converts a token list to its corresponding sentence (i.e. string)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgQduT1A7mm_"
      },
      "outputs": [],
      "source": [
        "# Setup helper functions for tokenizing and detokenizing sentences\n",
        "\n",
        "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Encodes a string to an array of integers\n",
        "\n",
        "    Args:\n",
        "        input_str (str): human-readable string to encode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "  \n",
        "    Returns:\n",
        "        numpy.ndarray: tokenized version of the input string\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "    \n",
        "    # Mark the end of the sentence with EOS\n",
        "    inputs = list(inputs) + [EOS]\n",
        "    \n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "    \n",
        "    return batch_inputs\n",
        "\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Decodes an array of integers to a human readable string\n",
        "\n",
        "    Args:\n",
        "        integers (numpy.ndarray): array of integers to decode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "  \n",
        "    Returns:\n",
        "        str: the decoded sentence.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Remove the dimensions of size 1\n",
        "    integers = list(np.squeeze(integers))\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # Remove the EOS to decode only the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)] \n",
        "    \n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK3Uj67g7mnA"
      },
      "source": [
        "Let's see how we might use these functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH05cqY57mnA",
        "outputId": "6034fb00-a3a3-426b-c1af-d622cd352fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mSingle detokenized example input:\u001b[0m Turns the spacebar function on or off.\n",
            "\u001b[31mSingle detokenized example target:\u001b[0m Activează sau dezactivează funcţia Spaţiu.\n",
            "\n",
            "\u001b[32mtokenize('hello'): \u001b[0m [[11487   360     1]]\n",
            "\u001b[32mdetokenize([11487, 360, 1]): \u001b[0m hello\n"
          ]
        }
      ],
      "source": [
        "# As declared earlier:\n",
        "# VOCAB_FILE = 'ende_32k.subword'\n",
        "# VOCAB_DIR = 'data/'\n",
        "\n",
        "# Detokenize an input-target pair of tokenized sentences\n",
        "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print()\n",
        "\n",
        "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
        "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
        "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f\"detokenize([11487, 360, 1]): \", 'green'), detokenize([11487, 360, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzyMebc-7mnB"
      },
      "source": [
        "\n",
        "## Bucketing\n",
        "\n",
        "Bucketing the tokenized sentences is an important technique used to speed up training in NLP.\n",
        "Here is a \n",
        "[nice article describing it in detail](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)\n",
        "but the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket, as on this image (from the article above):\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png)\n",
        "\n",
        "We batch the sentences with similar length together (e.g. the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences.\n",
        "In Trax, it is implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJXr-AIe7mnB"
      },
      "outputs": [],
      "source": [
        "# Bucketing to create streams of batches.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
        "\n",
        "# Create the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_eval_stream)\n",
        "\n",
        "# Add masking for the padding (0s).\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE4ioRvK7mnD"
      },
      "source": [
        "<a name=\"1.5\"></a>\n",
        "## 1.5  Exploring the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqrv7Xo57mnD",
        "outputId": "8d26e8b6-9ed0-48b7-efdf-847e2c2e3fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (16, 128)\n",
            "target_batch shape:  (16, 128)\n"
          ]
        }
      ],
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRJ4CIzZ7mnD"
      },
      "source": [
        "The `input_batch` and `target_batch` are Numpy arrays consisting of tokenized English sentences and Romanian sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjGXfEzq7mnE",
        "outputId": "66965bc8-7957-4ea1-d747-d0ef307aab1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
            "\u001b[0m Its request now takes the form of a 'reasoned opinion' under EU infringement procedures. \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            " \u001b[0m [ 2220   853   262  1702     2   913     5     9  3584 11089  4944   205\n",
            "  1478   204   210    25  7896 11423  1248  1539     4     1     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0] \n",
            "\n",
            "\u001b[31mTHIS IS THE ROMANIAN TRANSLATION: \n",
            "\u001b[0m Cererea ia în prezent forma unui „aviz motivat” în cadrul procedurilor UE de constatare a neîndeplinirii obligațiilor. \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ROMANIAN TRANSLATION: \n",
            "\u001b[0m [ 4464  8895     9  2793 15481  7038  4662 12618   292  3720  8435   261\n",
            "  9950     9  2897 12811    23  5762 15481 15427  4286 12618    23  9046\n",
            "  2077 11268    37 15481 15427  5752    64 15481  7038  4662 12618   292\n",
            "  7383 13061    23  7153  4186  8460  4955    23  2022  2885   670  5306\n",
            "  9529    38     9  3410 15481  7038  4662 12618  4533  7969  5876  9862\n",
            "   458  5860  6677  8127 15481  4665 15442 12618  6258  5547   120     4\n",
            "     1     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# pick a random index less than the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "# use the index to grab an entry from the input and target batch\n",
        "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
        "print(colored('THIS IS THE ROMANIAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ROMANIAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-YJa1ZZ7mnF"
      },
      "source": [
        "Neural Machine Translation with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laqOyuMe7mnG"
      },
      "source": [
        "\n",
        "\n",
        "The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a [Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) network which uses:\n",
        "\n",
        "   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n",
        "  \n",
        "   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the `n_encoder_layers` parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsWQJ7vY7mnG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "    \"\"\" Input encoder runs on the input sentence and creates\n",
        "    activations that will be the keys and values for attention.\n",
        "    \n",
        "    Args:\n",
        "        input_vocab_size: int: vocab size of the input\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    Returns:\n",
        "        tl.Serial: The input encoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    input_encoder = tl.Serial( \n",
        "        \n",
        "\n",
        "        # create an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(vocab_size=input_vocab_size, d_feature=d_model),\n",
        "        \n",
        "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n",
        "        [tl.LSTM(n_units=d_model) for _ in range(n_encoder_layers)]\n",
        "\n",
        "    )\n",
        "\n",
        "    return input_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6qFgJQF7mnI"
      },
      "source": [
        "###  Pre-attention decoder\n",
        "\n",
        "The pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:\n",
        "\n",
        "   - [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): This pads a token to the beginning of your target tokens (e.g. `[8, 34, 12]` shifted right is `[0, 8, 34, 12]`). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing.\n",
        "\n",
        "   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.\n",
        "html#trax.layers.core.Embedding): Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n",
        "\n",
        "   \n",
        "   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHF1z5Pe7mnI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
        "    activations that are used as queries in attention.\n",
        "    \n",
        "    Args:\n",
        "        mode: str: 'train' or 'eval'\n",
        "        target_vocab_size: int: vocab size of the target\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    Returns:\n",
        "        tl.Serial: The pre-attention decoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "        \n",
        "\n",
        "        # shift right to insert start-of-sentence token and implement\n",
        "        # teacher forcing during training\n",
        "        tl.ShiftRight(mode=mode),\n",
        "\n",
        "        # run an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(vocab_size=target_vocab_size, d_feature=d_model),\n",
        "\n",
        "        # feed to an LSTM layer\n",
        "        tl.LSTM(n_units=d_model)\n",
        "\n",
        "    )\n",
        "    \n",
        "    return pre_attention_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp6wid497mnK"
      },
      "source": [
        "### Preparing the attention input\n",
        "\n",
        "This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ7lUCB-7mnK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
        "    \n",
        "    Args:\n",
        "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
        "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
        "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
        "    \n",
        "    Returns:\n",
        "        queries, keys, values and mask for attention.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # set the keys and values to the encoder activations\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "\n",
        "    \n",
        "    # set the queries to the decoder activations\n",
        "    queries = decoder_activations\n",
        "    \n",
        "    # generate the mask to distinguish real tokens from padding\n",
        "\n",
        "    mask = (inputs != 0)\n",
        "\n",
        "    \n",
        "    # add axes to the mask for attention heads and decoder length.\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "    \n",
        "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
        "    # note: for this assignment, attention heads is set to 1.\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "        \n",
        "    \n",
        "    return queries, keys, values, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq_i7rbX7mnL"
      },
      "source": [
        "\n",
        "## Implementation Overview\n",
        "\n",
        "We are now ready to implement our sequence-to-sequence model with attention. .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfqgiaNg7mnM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
        "\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
        "    an English sentence (tokenized) and its translation into Romanian (tokenized).\n",
        "\n",
        "    Args:\n",
        "    input_vocab_size: int: vocab size of the input\n",
        "    target_vocab_size: int: vocab size of the target\n",
        "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
        "    n_attention_heads: int: number of attention heads\n",
        "    attention_dropout: float, dropout for the attention layer\n",
        "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
        "\n",
        "    Returns:\n",
        "    A LSTM sequence-to-sequence model with attention.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \n",
        "    # Step 0: call the helper function to create layers for the input encoder\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
        "\n",
        "    # Step 0: call the helper function to create layers for the pre-attention decoder\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
        "\n",
        "    # Step 1: create a serial network\n",
        "    model = tl.Serial( \n",
        "        \n",
        "      # Step 2: copy input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "        \n",
        "      # Step 3: run input encoder on the input and pre-attention decoder the target.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "        \n",
        "      # Step 4: prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "        \n",
        "      # Step 5: run the AttentionQKV layer\n",
        "      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
        "      \n",
        "      # Step 6: drop attention mask (i.e. index = None\n",
        "      tl.Select([0, 2]),\n",
        "        \n",
        "      # Step 7: run the rest of the RNN decoder\n",
        "      [tl.LSTM(n_units=d_model) for _ in range(n_decoder_layers)],\n",
        "        \n",
        "      # Step 8: prepare output by making it the right size\n",
        "      tl.Dense(target_vocab_size),\n",
        "        \n",
        "      # Step 9: Log-softmax for output\n",
        "      tl.LogSoftmax()\n",
        "    )\n",
        "    \n",
        "   \n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDmsIWRn7mnN",
        "outputId": "36964e23-7bc2-4293-dd03-13dd3e098c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial_in2_out2[\n",
            "  Select[0,1,0,1]_in2_out4\n",
            "  Parallel_in2_out2[\n",
            "    Serial[\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "    Serial[\n",
            "      Serial[\n",
            "        ShiftRight(1)\n",
            "      ]\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "  ]\n",
            "  PrepareAttentionInput_in3_out4\n",
            "  Serial_in4_out2[\n",
            "    Branch_in4_out3[\n",
            "      None\n",
            "      Serial_in4_out2[\n",
            "        _in4_out4\n",
            "        Serial_in4_out2[\n",
            "          Parallel_in3_out3[\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "          ]\n",
            "          PureAttention_in4_out2\n",
            "          Dense_1024\n",
            "        ]\n",
            "        _in2_out2\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Select[0,2]_in3_out2\n",
            "  LSTM_1024\n",
            "  LSTM_1024\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# print your model\n",
        "model = NMTAttn()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbW6S12Z7mnO"
      },
      "source": [
        "\n",
        "#  Training\n",
        "\n",
        "Now be training our model in this section. Doing supervised training in Trax is pretty straightforward (short example [here](https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training)). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BufWxTmw7mnP"
      },
      "source": [
        "\n",
        "## TrainTask\n",
        "\n",
        "The [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2rr01lL7mnP"
      },
      "outputs": [],
      "source": [
        " \n",
        "train_task = training.TrainTask(\n",
        "\n",
        "    \n",
        "    # use the train batch stream as labeled data\n",
        "    labeled_data= train_batch_stream,\n",
        "    \n",
        "    # use the cross entropy loss\n",
        "    loss_layer= tl.CrossEntropyLoss(),\n",
        "    \n",
        "    # use the Adam optimizer with learning rate of 0.01\n",
        "    optimizer= trax.optimizers.Adam(0.01),\n",
        "    \n",
        "    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule\n",
        "    # have 1000 warmup steps with a max value of 0.01\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "    \n",
        "    # have a checkpoint every 10 steps\n",
        "    n_steps_per_checkpoint= 10,\n",
        "    \n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltD0ncz87mnQ"
      },
      "source": [
        "\n",
        "## EvalTask\n",
        "\n",
        "The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxy44f1w7mnQ"
      },
      "outputs": [],
      "source": [
        "eval_task = training.EvalTask(\n",
        "    \n",
        "    ## use the eval batch stream as labeled data\n",
        "    labeled_data=eval_batch_stream,\n",
        "    \n",
        "    ## use the cross entropy loss and accuracy as metrics\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry0W76NC7mnR"
      },
      "source": [
        "\n",
        "## Loop\n",
        "\n",
        "The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "MKCIhg4k7mnS"
      },
      "outputs": [],
      "source": [
        "# define the output directory\n",
        "output_dir = 'output_dir/'\n",
        "\n",
        "# remove old model if it exists. restarts training.\n",
        "!rm -f ~/output_dir/model.pkl.gz  \n",
        "\n",
        "# define the training loop\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmvaeJvQ7mnT",
        "outputId": "179f6353-b666-4072-98c7-2878b66ef6ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step    220: Ran 9 train steps in 451.88 secs\n",
            "Step    220: train CrossEntropyLoss |  5.83641434\n",
            "Step    220: eval  CrossEntropyLoss |  6.02568960\n",
            "Step    220: eval          Accuracy |  0.05886525\n",
            "\n",
            "Step    230: Ran 10 train steps in 506.62 secs\n",
            "Step    230: train CrossEntropyLoss |  6.01977491\n",
            "Step    230: eval  CrossEntropyLoss |  6.11215210\n",
            "Step    230: eval          Accuracy |  0.06151833\n",
            "\n",
            "Step    240: Ran 10 train steps in 548.62 secs\n",
            "Step    240: train CrossEntropyLoss |  5.83679008\n",
            "Step    240: eval  CrossEntropyLoss |  5.97921848\n",
            "Step    240: eval          Accuracy |  0.06138473\n"
          ]
        }
      ],
      "source": [
        "\n",
        "training_loop.run(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJKM5jgG7mnU"
      },
      "source": [
        "\n",
        "# Testing\n",
        "\n",
        "We will now be using the model you just trained to translate English sentences to Romanian. We will implement this with two functions: The first allows you to identify the next symbol (i.e. output token). The second one takes care of combining the entire translated string.\n",
        "\n",
        "We will start by first loading in a pre-trained copy of the model you just coded. Please run the cell below to do just that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i98hGgYA7mnU"
      },
      "outputs": [],
      "source": [
        "# instantiate the model we built in eval mode\n",
        "model = NMTAttn(mode='eval')\n",
        "\n",
        "# initialize weights from a pre-trained model\n",
        "model.init_from_file(\"/content/output_dir/model.pkl.gz\", weights_only=True)\n",
        "model = tl.Accelerate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AydrDMEA7mnV"
      },
      "source": [
        "\n",
        "##  Decoding\n",
        "\n",
        "There are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e. greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0G4tlZ17mnV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
        "    \"\"\"Returns the index of the next token.\n",
        "\n",
        "    Args:\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        int: index of the next token in the translated sentence\n",
        "        float: log probability of the next symbol\n",
        "    \"\"\"\n",
        "\n",
        "    # set the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "\n",
        "    # calculate next power of 2 for padding length \n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1))) \n",
        "\n",
        "    # pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
        "    \n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # convert `padded` list to a numpy array with shape (None, <padded_length>) where\n",
        "    # None is a placeholder for the batch size\n",
        "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
        "    \n",
        "    # get the model prediction (remember to use the `NMAttn` argument defined above)\n",
        "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
        "\n",
        "    # get log probabilities from the last token output\n",
        "    log_probs = output[0, token_length, :]\n",
        "    \n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "\n",
        "    return symbol, float(log_probs[symbol])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aLWxj6Q7mnX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (str): sentence to translate.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list, str, float)\n",
        "            list of int: tokenized version of the translated sentence\n",
        "            float: log probability of the translated sentence\n",
        "            str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    \n",
        "    # encode the input sentence\n",
        "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\n",
        "    \n",
        "    # initialize the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "    \n",
        "    # initialize an integer that represents the current output index\n",
        "    cur_output = 0\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # check that the current output is not the end of sentence token\n",
        "    while cur_output != EOS:\n",
        "        print(cur_output)\n",
        "        # update the current output token by getting the index of the next word (hint: use next_symbol)\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
        "        \n",
        "        # append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output)\n",
        "    \n",
        "    # detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
        "    \n",
        "    \n",
        "    return cur_output_tokens, log_prob, sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sampling_decode(\"I love languages.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "metadata": {
        "id": "MbzlFliRzO8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO2Rhzi57mnZ"
      },
      "outputs": [],
      "source": [
        "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Prints the input and output of our NMTAttn model using greedy decode\n",
        "\n",
        "    Args:\n",
        "        sentence (str): a custom string.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    \n",
        "    print(\"English: \", sentence)\n",
        "    print(\"Romaniany: \", translated_sentence)\n",
        "    \n",
        "    return translated_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put a custom string here\n",
        "your_sentence = 'Hello.'\n",
        "\n",
        "greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "metadata": {
        "id": "-ltHUcvizR7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zesXEou7mna"
      },
      "outputs": [],
      "source": [
        "greedy_decode_test('Test your translation!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgjlBsMg7mnb"
      },
      "source": [
        "\n",
        "## Minimum Bayes-Risk Decoding\n",
        "\n",
        "Ggetting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
        "\n",
        "1. take several random samples\n",
        "2. score each sample against all other samples\n",
        "3. select the one with the highest score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha4gfzXU7mnb"
      },
      "source": [
        "\n",
        "### Generating samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7BX9Ab67mnc"
      },
      "outputs": [],
      "source": [
        "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Generates samples using sampling_decode()\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (list, list)\n",
        "            list of lists: token list per sample\n",
        "            list of floats: log probability per sample\n",
        "    \"\"\"\n",
        "    # define lists to contain samples and probabilities\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    # run a for loop to generate n samples\n",
        "    for _ in range(n_samples):\n",
        "        \n",
        "        # get a sample using the sampling_decode() function\n",
        "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "        \n",
        "        # append the token list to the samples list\n",
        "        samples.append(sample)\n",
        "        \n",
        "        # append the log probability to the log_probs list\n",
        "        log_probs.append(logp)\n",
        "                \n",
        "    return samples, log_probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate 4 samples with the default temperature (0.6)\n",
        "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "metadata": {
        "id": "aVvdFewTzUrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voKkVGqe7mnd"
      },
      "source": [
        "### Comparing overlaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dqgkPPo7mnd"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "    \"\"\"Returns the Jaccard similarity between two token lists\n",
        "\n",
        "    Args:\n",
        "        candidate (list of int): tokenized version of the candidate translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert the lists to a set to get the unique tokens\n",
        "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
        "    \n",
        "    # get the set of tokens common to both candidate and reference\n",
        "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "    \n",
        "    # get the set of all tokens found in either candidate or reference\n",
        "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "    \n",
        "    # divide the number of joint elements by the number of all elements\n",
        "    overlap = len(joint_elems) / len(all_elems)\n",
        "    \n",
        "    return overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy_hrZKw7mnd",
        "outputId": "605066f7-9f99-490e-d863-f30d821e8959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# let's try using the function. remember the result here and compare with the next function below.\n",
        "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVMjOLBc7mne"
      },
      "source": [
        "One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in class, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B6xUd4C7mne"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# for making a frequency table easily\n",
        "from collections import Counter\n",
        "\n",
        "def rouge1_similarity(system, reference):\n",
        "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
        "\n",
        "    Args:\n",
        "        system (list of int): tokenized version of the system translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"    \n",
        " \n",
        "    \n",
        "    # make a frequency table of the system tokens (hint: use the Counter class)\n",
        "    sys_counter = Counter(system)\n",
        "    \n",
        "    # make a frequency table of the reference tokens (hint: use the Counter class)\n",
        "    ref_counter = Counter(reference)\n",
        "    \n",
        "    # initialize overlap to 0\n",
        "    overlap = 0\n",
        "    \n",
        "    # run a for loop over the sys_counter object (can be treated as a dictionary)\n",
        "    for token in sys_counter:\n",
        "        \n",
        "        # lookup the value of the token in the sys_counter dictionary (hint: use the get() method)\n",
        "        token_count_sys = sys_counter.get(token, 0)\n",
        "        \n",
        "        # lookup the value of the token in the ref_counter dictionary (hint: use the get() method)\n",
        "        token_count_ref = ref_counter.get(token, 0)\n",
        "        \n",
        "        # update the overlap by getting the smaller number between the two token counts above\n",
        "        overlap += min(token_count_sys, token_count_ref)\n",
        "    \n",
        "    # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
        "    precision = overlap / sum(sys_counter.values())\n",
        "    \n",
        "    # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
        "    recall = overlap / sum(ref_counter.values())\n",
        "    \n",
        "    if precision + recall != 0:\n",
        "        # compute the f1-score\n",
        "        rouge1_score = rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
        "    else:\n",
        "        rouge1_score = 0 \n",
        "\n",
        "    \n",
        "    return rouge1_score\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yFw2WGl7mnf",
        "outputId": "12b78bf0-ff15-46b8-baee-521e7cea7818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# notice that this produces a different value from the jaccard similarity earlier\n",
        "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryNNNimK7mng"
      },
      "source": [
        " Overall score\n",
        "\n",
        "We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "1. Get similarity score between sample 1 and sample 2\n",
        "2. Get similarity score between sample 1 and sample 3\n",
        "3. Get similarity score between sample 1 and sample 4\n",
        "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
        "5. Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV25wnH67mng"
      },
      "outputs": [],
      "source": [
        "\n",
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        similarity_fn (function): similarity function used to compute the overlap\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        *ignore_params: additional parameters will be ignored\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"  \n",
        "    \n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "\n",
        "        \n",
        "        # initialize overlap to 0.0\n",
        "        overlap = 0.0\n",
        "        \n",
        "        # run a for loop for each sample\n",
        "        for index_sample, sample in enumerate(samples): \n",
        "\n",
        "            # skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            \n",
        "            # add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap\n",
        "            \n",
        "        # get the score for the candidate by computing the average\n",
        "        score = overlap/index_sample\n",
        "        \n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sprijTgr7mnh",
        "outputId": "5a7c73a3-3ca0-4299-c1a2-1046a585aec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.45, 1: 0.625, 2: 0.575}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWj4QYO7mni"
      },
      "source": [
        "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMd-XkOD7mni"
      },
      "outputs": [],
      "source": [
        "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
        "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        log_probs (list of float): log probability of the translated sentences\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        \n",
        "        # initialize overlap and weighted sum\n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "        \n",
        "        # run a for loop for each sample\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "\n",
        "            # skip if the candidate index is the same as the sample index            \n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # convert log probability to linear scale\n",
        "            sample_p = float(np.exp(logp))\n",
        "\n",
        "            # update the weighted sum\n",
        "            weight_sum += sample_p\n",
        "\n",
        "            # get the unigram overlap between candidate and sample\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            \n",
        "            # update the overlap\n",
        "            overlap += sample_p * sample_overlap\n",
        "            \n",
        "        # get the score for the candidate\n",
        "        score = overlap / weight_sum\n",
        "        \n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV8LgQPm7mnj",
        "outputId": "a25d1d8a-df88-471b-f561-765cf96a14f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FwCOeKZ7mnk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        score_fn (function): function that generates the score for each sample\n",
        "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # generate samples\n",
        "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
        "    \n",
        "    # use the scoring function to get a dictionary of scores\n",
        "    # pass in the relevant parameters as shown in the function definition of \n",
        "    # the mean methods you developed earlier\n",
        "    scores = score_fn(similarity_fn, samples, log_probs)\n",
        "    \n",
        "    # find the key with the highest score\n",
        "    max_index = max(scores, key=scores.get)\n",
        "    \n",
        "    # detokenize the token list associated with the max_index\n",
        "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
        "    \n",
        "\n",
        "    return (translated_sentence, max_index, scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHGj1uv7mnl"
      },
      "outputs": [],
      "source": [
        "TEMPERATURE = 1.0\n",
        "\n",
        "# put a custom string here\n",
        "your_sentence = 'She speaks English and German.'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "metadata": {
        "id": "WPp2ivD2zj51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mbr_decode('Congratulations!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "metadata": {
        "id": "b5AwjySlzmbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "metadata": {
        "id": "e3iugvZqzpMR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-1"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "AutomatedTranslation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}